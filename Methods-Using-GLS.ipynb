{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs all the different methods using GLS. Takes in y, U, X, gamma, beta, CovBeta, ChiSq\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import xlrd\n",
    "import xlwt\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from scipy.stats import chi2\n",
    "import operator as op\n",
    "import itertools\n",
    "from GTC import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"Read-data.ipynb\" -G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MP(y, U, w, X, gamma, beta, CovBeta, ChiSq, ChiEx, w_OO, method):\n",
    "    # Mandel-Paule (with no cut off)\n",
    "    # Need to specify whether using model A or model B to calculate chisq\n",
    "    # Returns: estimates after MP (beta), uncertainties in these estimates (CovBeta), uncertainty added (U_adjust)\n",
    " \n",
    "    U_adjust = 0\n",
    "    unc_y = np.sqrt(np.matrix(U).diagonal(offset=0, axis1=0, axis2=1))\n",
    "    \n",
    "    # Initalise values for gamma_MP, beta_MP, CovBeta_MP, ChiSq, w_MP to the values of them before MP is applied (these are \n",
    "    # used if there is no change in the valuess after MP)\n",
    "    gamma_MP = gamma*1.\n",
    "    beta_MP = beta*1.\n",
    "    CovBeta_MP = CovBeta*1.\n",
    "    w_MP = w*1.\n",
    "    \n",
    "    # Initialise average uncertainty\n",
    "    ave_U_MP = ave_U * 1.\n",
    "    \n",
    "    # If ChiSq is greater than the expected value, adjust the uncertainty until it is within tolerance of the expected value\n",
    "    if ChiSq > ChiEx+0.02:\n",
    "    \n",
    "        nsteps = 0\n",
    "        nsteps2 = 0\n",
    "\n",
    "        ChiDiff_Orig = ChiSq - ChiEx\n",
    "        ChiDiff =  ChiSq - ChiEx\n",
    "        U_adjust = ChiDiff*unc_y[0,0]/1000.\n",
    "            \n",
    "        # Continue iteration until we get chisq within 0.01 of target\n",
    "        while abs(ChiDiff) > 0.01 and nsteps2 < 500:\n",
    "            if nsteps > 500:\n",
    "                U_adjust = U_adjust*10.\n",
    "                nsteps = 0\n",
    "\n",
    "            nsteps += 1\n",
    "            nsteps2 += 1    \n",
    "\n",
    "    \n",
    "            # Add adjustment to the uncertainty and recalculate weights, artefact values, DOEs and chisq.\n",
    "            adj = np.repeat(U_adjust**2, NumMeasurements)\n",
    "            \n",
    "            # Add the adjustment to the diagonal entries of U for calculating beta\n",
    "            U_MP = np.diag(adj) + U \n",
    "            ave_U_MP = ave_U + np.repeat(U_adjust, NumParticipants) \n",    
    "            \n",
    "            # Update the weights to fit with the new uncertainty values\n",
    "            for i in range(0, NumParticipants):\n",
    "                w_MP[NumArtefacts+i, 0] = 1/(ave_U_MP[i]**2)\n",
    "\n",
    "            # Standardise the weights so they sum to 1\n",
    "            w_MP = w_MP/sum(w_MP)\n",
    "            \n",
    "            # Recalculate Beta,CovBeta, then using the right method            \n",
    "            [gamma_MP, beta_MP, CovBeta_MP] = calculate_beta(y,U_MP,X,w_MP)\n",
    "            \n",
    "            if method == 'A':\n",
    "                ChiSq = model_A(beta_MP, ave_U_MP)\n",
    "            elif method == 'B':\n",
    "                ChiSq = model_B(y,U_MP,X,beta_MP)\n",
    "            \n",
    "            ChiDiff = ChiSq - ChiEx\n",
    "            \n",
    "            # Update U_adjust\n",
    "            U_adjust = U_adjust*np.sqrt(abs(ChiDiff_Orig/(ChiDiff_Orig-ChiDiff)))\n",
    "        \n",
    "    return(beta_MP, CovBeta_MP, U_adjust, ChiSq, w_MP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MP_C(y, U, w, X, gamma, beta, CovBeta, ChiSq, ChiEx, w_OO_C, method):\n",
    "    # Mandel-Paule (with cut off) - same as above but applying the cut-off as well\n",
    "    # Need to specify whether using model A or model B to calculate chisq\n",
    "    # Returns: estimates after MP and cut off have been applied (beta), uncertainties in these estimates (CovBeta)\n",
    "    \n",
    "    # Initialise values:\n",
    "    U_adjust = 0\n",
    "    unc_y = np.sqrt(np.matrix(U).diagonal(offset=0, axis1=0, axis2=1))\n",
    "    \n",
    "    # Set values for gamma_MP_C, beta_MP_C, CovBeta_MP_C, ChiSq (these are used if there is no change in the valuess after MP)\n",
    "    gamma_MP_C = gamma*1.\n",
    "    beta_MP_C = beta*1.\n",
    "    CovBeta_MP_C = CovBeta*1.\n",
    "    w_MP_C = w*1.\n",
    "    ave_U_MP_C = ave_U *1.\n",
    "    \n",
    "    # If ChiSq is greater than the expected value, adjust the uncertainty until it is within tolerance of the expected value\n",
    "    if ChiSq > ChiEx+0.02:\n",
    "    \n",
    "        nsteps = 0\n",
    "        nsteps2 = 0\n",
    "\n",
    "        ChiDiff_Orig = ChiSq - ChiEx\n",
    "        ChiDiff =  ChiSq - ChiEx\n",
    "        U_adjust = ChiDiff*unc_y[0,0]/1000.\n",
    "            \n",
    "        # Continue iteration until get chisq within 0.01 of target\n",
    "        while abs(ChiDiff) > 0.01 and nsteps2 < 500:\n",
    "            if nsteps > 500:\n",
    "    \n",
    "                U_adjust = U_adjust*10.\n",
    "                nsteps = 0\n",
    "\n",
    "            nsteps += 1\n",
    "            nsteps2 += 1\n",
    "\n",
    "            # Add adjustment and recalculate weights, artefact values, DOEs and chisq.\n",
    "            adj = np.repeat(U_adjust**2, NumMeasurements)\n",
    "            \n",
    "            # add the adjustment to the diagonal entries of U\n",
    "            U_MP = np.diag(adj) + U             \n",
    "            \n",
    "            ave_U_MP_C += np.repeat(U_adjust, NumParticipants)\n",
    "\n",
    "            for k in range(0,NumParticipants):\n",
    "                w_MP_C[NumArtefacts+k, 0] = 1/ave_U_MP_C[k]\n",
    "    \n",
    "            # Standardise the weights so they sum to 1\n",
    "            w_MP_C = w_MP_C/sum(w_MP_C)\n",
    "            \n",
    "            # Apply cut off\n",
    "            [w_MP_C,cut,u_cut] = cut_off(ave_U_MP_C, w)\n",
    "\n",
    "            [gamma_MP_C, beta_MP_C, CovBeta_MP_C] = calculate_beta(y,U_MP,X,w_MP_C)\n",
    "\n",
    "            if method == 'A':\n",
    "                ChiSq = model_A(beta_MP_C, ave_U_MP_C)\n",
    "            elif method == 'B':\n",
    "                ChiSq = model_B(y,U_MP,X,beta_MP_C)\n",
    "                \n",
    "            ChiDiff = ChiSq - ChiEx\n",
    "            \n",
    "            # Update U_adjust\n",
    "            U_adjust = U_adjust*np.sqrt(abs(ChiDiff_Orig/(ChiDiff_Orig-ChiDiff)))\n",
    "          \n",
    "    return(beta_MP_C,CovBeta_MP_C, U_adjust, ChiSq, w_MP_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OO(y, U, w, X, gamma, beta, CovBeta, ChiSq):\n",
    "    # Obvious outliers\n",
    "    # Returns: weights for OO which are 0 for excluded labs and 1 for all others.\n",
    "\n",
    "    # Calculate distance\n",
    "    cov_beta_diag = np.matrix(CovBeta[NumArtefacts:,NumArtefacts:]).diagonal(offset=0, axis1=0, axis2=1)\n",
    "    d = beta[NumArtefacts:]\n",
    "    dist = d/(6*np.sqrt(cov_beta_diag.T))\n",
    "    \n",
    "    # Create array to store the weights (0 for excluded labs, 1 for the rest)\n",
    "    w_cs = np.ones((NumParticipants+NumArtefacts,1))\n",
    "    n = 0\n",
    "    \n",
    "    # Iterate through measurements, and if distance > 1, set weight for that lab to 0\n",
    "    for i in range(0,NumParticipants):\n",
    "        if abs(dist[i]) > 1.0:\n",
    "            w_cs[i+NumArtefacts,0] = 0.\n",
    "            n += 1\n",
    "    \n",
    "    # If some weighting changes, update weights and recalculate chisq, gamma, beta, covbeta\n",
    "    if n > 0: \n",
    "        # set the weights for the excluded labs to 0.\n",
    "        wp = w_cs * w\n",
    "        \n",
    "        # standardise the weights\n",
    "        w_OO = wp / sum(wp) \n",
    "                \n",
    "        # Recalculate chisq (model B)\n",
    "        ChiSq_OO = model_B(y,U,X,w_OO)\n",
    "    \n",
    "    else:\n",
    "        w_OO = w * 1.\n",
    "\n",
    "    return(w_OO, w_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OO_C(y, U, w, X, gamma, beta, CovBeta, ChiSq):\n",
    "    # Obvious outliers with cut-off\n",
    "    # Returns: weights for OO_C which are 0 for excluded labs and 1 for all others.\n",
    "    \n",
    "    # Initialise w_OO_C to the value of w\n",
    "    w_OO_C = w*1.\n",
    "\n",
    "    # Calculate differences\n",
    "    cov_beta_diag = np.matrix(CovBeta[NumArtefacts:,NumArtefacts:]).diagonal(offset=0, axis1=0, axis2=1)\n",
    "    d = beta[NumArtefacts:]\n",
    "    dist = d/(6*np.sqrt(cov_beta_diag.T))\n",
    "    \n",
    "    # Create array to store the weights (0 for excluded labs, 1 for the rest)\n",
    "    w_cs = np.ones((NumParticipants+NumArtefacts,1))\n",
    "    n = 0\n",
    "    \n",
    "    # Iterate through measurements, and if distance > 1, set weight for that lab to 0\n",
    "    for i in range(0,NumParticipants):\n",
    "        if abs(dist[i]) > 1.0:\n",
    "            w_cs[i+NumArtefacts,0] = 0.\n",
    "            n += 1\n",
    "    \n",
    "    # If some weighting changes, update weights and recalculate chisq, gamma, beta, covbeta\n",
    "    if n > 0:\n",
    "        \n",
    "        # Calculate weights with cutoff\n",
    "        [w_OO_C,cut,u_cut] = cut_off(ave_U, w)\n",
    "\n",
    "        # Recalculate chisq (model B)\n",
    "        ChiSq_OO_C = model_B(y,U,X,w_OO_C)\n",
    "\n",
    "    return (w_OO_C, w_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_off(U_ave,w):\n",
    "    # Calculate weights with cut-off.\n",
    "    # Returns: a vector of weights updated after applying the cutoff, u_cut\n",
    "    \n",
    "    # Set u_vector to the average uncertainties\n",
    "    u_vector = U_ave *1.\n",
    "    \n",
    "    u_cut = np.zeros(len(u_vector))\n",
    "    \n",
    "    # Sort uncertainties from smallest to largest\n",
    "    temp = np.sort(u_vector) \n",
    "    \n",
    "    # Find average of smallest half\n",
    "    cut = np.average(temp[0:round(NumParticipants/2)]) \n",
    "    \n",
    "    for i in range(0,len(u_vector)):\n",
    "        if u_vector[i] < cut:\n",
    "            u_cut[i] = cut       \n",
    "        else:\n",
    "            u_cut[i] = u_vector[i]\n",
    "\n",
    "    # Standardise so weights add to 1\n",
    "    w_C = np.zeros((len(w),1))\n",
    "    for i in range(0,len(w)):\n",
    "        if i >= NumArtefacts:\n",
    "            w_C[i,0] = 1/u_cut[i-NumArtefacts]**2\n",
    "       \n",
    "    wC = w_C/sum(w_C)\n",
    "    c = np.mean(np.linalg.multi_dot([X.T,linalg.inv(U),X]))/np.mean(np.dot(wC,wC.T))\n",
    "    wC = np.sqrt(c)*wC\n",
    "\n",
    "    return(wC,u_cut,cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoCut(y, U, w, X, gamma, beta, CovBeta, ChiSq, method, include_MP=False, include_OO=False):\n",
    "    # Applies the model, options to apply OO then MP\n",
    "    # Need to specify which model is being used to calculate chisq\n",
    "    # Returns: estimates of the variables (beta_MP), and their uncertainties (CovBeta_MP)\n",
    "    \n",
    "    # Determine the expected value of ChiSq\n",
    "    if method == 'B':\n",
    "        ChiEx = chi2.isf(0.05,NumMeasurements+1-NumArtefacts-NumParticipants)\n",
    "    else:\n",
    "        ChiEx = chi2.isf(0.05,NumParticipants-1)\n",
    "    \n",
    "    \n",
    "    # Apply obvious outliers if the value of OO is true, otherwise save w_OO_C as an array of ones\n",
    "    if include_OO == True:\n",
    "        # Check for 'obvious outliers' and recalculate if necc\n",
    "        [w_OO, w_cs] = OO(y, U, w, X, gamma, beta, CovBeta, ChiSq)\n",
    "    else:\n",
    "        w_OO = 1.*w\n",
    "        w_cs = np.ones(NumParticipants)\n",
    "        \n",
    "    \n",
    "    # Apply Mandel-Paule if the value of MP is true, otherwise calculate beta_MP and CovBeta_MP from model B\n",
    "    if include_MP == True:\n",
    "        # Check for consistency and apply MP if necc\n",
    "        [beta_MP, CovBeta_MP, U_adjust, ChiSq_MP, w_MP] = MP(y, U, w, X, gamma, beta, CovBeta, ChiSq, ChiEx, w_OO, method)\n",
    "    else:\n",
    "        [gamma_MP,beta_MP,CovBeta_MP] = calculate_beta(y,U,X,w_OO)\n",
    "        U_adjust = 0\n",
    "        ChiSq_MP = ChiSq * 1.\n",
    "        w_MP = w_OO*1.\n",
    "\n",
    "        \n",
    "    return(beta_MP, CovBeta_MP, U_adjust, w_cs, ChiSq_MP, w_MP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cut(y, U, w, X, gamma, beta, CovBeta, ChiSq,ave_U, method, include_MP=False, include_OO=False):\n",
    "    # Applies the cut-off, options to also apply OO then MP\n",
    "    # Need to specify which model is being used to calculate chisq\n",
    "    # Returns: estimates of the variables (beta_MP), and their uncertainties (CovBeta_MP)\n",
    "    \n",
    "    # Apply cut-off and update weights\n",
    "    [w_C,cut,u_cut] = cut_off(ave_U,w)\n",
    "    \n",
    "    # Calculate gamma, beta, CovBeta, ChiSq after the cut-off\n",
    "    [gamma_C,beta_C,CovBeta_C] = calculate_beta(y,U,X,w_C)\n",
    "    \n",
    "    # Determine the expected value of ChiSq\n",
    "    if method == 'B':\n",
    "        ChiEx = chi2.isf(0.05,NumMeasurements+1-NumArtefacts-NumParticipants)\n",
    "        ChiSq_C = model_B(y, U, X, beta)\n",
    "    else:\n",
    "        ChiEx = chi2.isf(0.05,NumMeasurements-NumArtefacts)\n",
    "        [ChiSq_C] = model_A(beta, ave_U)\n",
    "    \n",
    "    \n",
    "    # Apply obvious outliars if the value of OO is true, otherwise save w_OO_C as an array of ones\n",
    "    if include_OO == True:\n",
    "        # Check for 'obvious outliers' and recalculate if necc\n",
    "        [w_OO_C, w_cs] = OO_C(y, U, w_C, X, gamma_C, beta_C, CovBeta_C, ChiSq_C)\n",
    "    else:\n",
    "        w_OO_C = 1.*w_C\n",
    "        w_cs = np.ones(NumParticipants)\n",
    "\n",
    "    \n",
    "    # Apply Mandel-Paule if the value of MP is true, otherwise calculate beta_MP_C and CovBeta_MP_C from model B\n",
    "    if include_MP == True:\n",
    "        # Check for consistency and apply MP if necc\n",
    "        [beta_MP_C,CovBeta_MP_C, U_adjust, ChiSq_MP, w_MP] = MP_C(y, U, w, X, gamma_C, beta_C, CovBeta_C, ChiSq_C, ChiEx, \n",
    "                                                                  w_OO_C, method)\n",
    "    else:\n",
    "        [gamma_MP_C,beta_MP_C,CovBeta_MP_C] = calculate_beta(y,U,X,w_OO_C)\n",
    "        U_adjust = 0\n",
    "        ChiSq_MP = ChiSq * 1.\n",
    "        w_MP = w_OO_C*1.\n",
    "\n",
    "    return (beta_MP_C, CovBeta_MP_C, U_adjust, w_cs, w_OO_C, ChiSq_MP, w_MP,cut,u_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LChS(beta, CovBeta):\n",
    "    # Largest coherent subset\n",
    "    # Returns: the size of the largest coherent subset in the data, t\n",
    "    \n",
    "    N = NumParticipants\n",
    "    \n",
    "    # Get DoEs and their uncertainties out of beta, CovBeta and save\n",
    "    delta = la.uarray([ureal(beta[i+NumArtefacts,0],np.sqrt(CovBeta[i+NumArtefacts,i+NumArtefacts]),\n",
    "                             independent=False) for i in range(0,N)])\n",
    "        \n",
    "    for i in range(0,N):\n",
    "        for j in range(0,N):\n",
    "            if i != j:\n",
    "                set_correlation((CovBeta[i+NumArtefacts,j+NumArtefacts]/(np.sqrt(CovBeta[i+NumArtefacts,i+NumArtefacts])*\n",
    "                                                                         np.sqrt(CovBeta[j+NumArtefacts,j+NumArtefacts]))),\n",
    "                                delta[i], delta[j])\n",
    "            else: \n",
    "                set_correlation(1, delta[i], delta[j])\n",
    "\n",
    "                \n",
    "    # Calculate the values of alpha_star and z\n",
    "    alpha_star = 2*(0.05/(N*(N-1)))\n",
    "    z = stats.norm.ppf(1-alpha_star/2)\n",
    "    \n",
    "    # Initialise I to store results. I will contain a 1 in the i,j th position if NMIs i,j are not equivalent, and a 0 if\n",
    "    # they are equivalent\n",
    "    I = np.ones(([N,N]))\n",
    "        \n",
    "    # Iterate through the NMIs twice\n",
    "    for i in range(0,N):\n",
    "        for k in range(0,N):  \n",
    "                \n",
    "            if i != k:\n",
    "                # If i,k correspond to different NMIs\n",
    "                    \n",
    "                # Calculate the lower and upper bounds to t\n",
    "                lower = value(delta[i] - delta[k]) - z*(uncertainty(delta[i] - delta[k]))\n",
    "                upper = value(delta[i] - delta[k]) + z*(uncertainty(delta[i] - delta[k]))\n",
    "                \n",
    "                # If lower < 0 < upper update I to show i,k are equivalent\n",
    "                if lower < 0 and upper > 0:\n",
    "                    I[i,k] = 0\n",
    "                \n",
    "            else:\n",
    "                # If we are considering the same NMI, then it is equivalent to itself so set I to 0\n",
    "                I[i,k] = 0\n",
    "    \n",
    "    \n",
    "    # Determine the largest coherent subset for each column\n",
    "    # Create dictionaries to store values, equiv for the NMIs which are equivalent pairwise\n",
    "    equiv = {}\n",
    "\n",
    "    for i in range(0,N):\n",
    "        # Iterate through the rows in the matrix\n",
    "        equiv[i] = []\n",
    "\n",
    "        for k in range(0,N):\n",
    "            # Iterate through the columns\n",
    "\n",
    "            if I[i][k]==0:\n",
    "                # If 0, add to a\n",
    "                equiv[i].append(k)\n",
    "        \n",
    "    lchs = 0 # to store maximum subset size\n",
    "        \n",
    "    for i in range(0,N):\n",
    "        # Iterate through the rows of the matrix\n",
    "        s = []\n",
    "        s.append(i)\n",
    "        rows = list(range(0,N))\n",
    "        stop = False\n",
    "    \n",
    "        while stop == False:\n",
    "            # Until we finish going through all rotations of the list\n",
    "                \n",
    "            # Iterate through the rows of the matrix and if the row is equivalent to all elements in s, add row j to s\n",
    "            for k in rows:\n",
    "                test = 0\n",
    "                for v in s:\n",
    "                    if k not in equiv[v]:\n",
    "                        test += 1\n",
    "                if test == 0:\n",
    "                    s.append(k)\n",
    "                \n",
    "            # Get rid of any repeats in s, and if its length is longer than the largest so far, save it\n",
    "            s2 = set(s)\n",
    "            if len(s2) >= lchs:\n",
    "                lchs = len(s2)\n",
    "        \n",
    "            # Move all the elements in the list rows around\n",
    "            rows = rows[1:] + [rows[0]]\n",
    "                \n",
    "            s = []\n",
    "            s.append(i)\n",
    "        \n",
    "            # If we have tried all possible starting elements, get out of loop\n",
    "            if rows[0] == 0:\n",
    "                stop = True  \n",
    "    \n",
    "    return(lchs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LCS(y, ave_U):\n",
    "    # Largest consistent subset\n",
    "    # Returns: an array containing ones corresponding to the measurements which are contained in the Largest consistent subset\n",
    "    # and zeros for the ones which are not.\n",
    "    \n",
    "    N = NumParticipants\n",
    "    S_N = set((i for i in range(0,N)))\n",
    "    w_set = np.zeros([N], dtype = int)\n",
    "    ChiSq = 0\n",
    "    d = np.empty([N],dtype = object)\n",
    "    theta = np.empty(1,dtype = object)\n",
    "    w = np.zeros((NumArtefacts+NumParticipants,1))\n",
    "    \n",
    "    NotDone = 0\n",
    "    uncertainties = ave_U*1.\n",
    "\n",
    "    t = N*1\n",
    "    NotDone = 0\n",
    "    while NotDone==0:\n",
    "        # List of all subsets of size t in set s of size N\n",
    "        ss = [set(i) for i in itertools.combinations(S_N, t)]\n",
    "        \n",
    "        # Convert to array of arrays\n",
    "        St =np.array([list(ss[i]) for i in range(0,len(ss))])\n",
    "        Q = len(ss)\n",
    "        \n",
    "        d_all = np.zeros([N,Q],dtype = object)\n",
    "        theta_all = np.zeros([Q],dtype = object)\n",
    "        chp = np.zeros([N,Q],dtype = float)\n",
    "        ChiSq_all = np.zeros([Q],dtype = float)\n",
    "        w_s = np.zeros([N+NumArtefacts,Q],dtype = int)\n",
    "    \n",
    "        for m in range(0,Q):\n",
    "            # Weighting vector for set of participants with zero bias\n",
    "            for i in range(0,t):      \n",
    "                w_s[NumArtefacts+St[m,i],m] = 1. \n",
    "            \n",
    "            # Calculate weights taking into account missing participants for this subset\n",
    "            wp = np.array([w_s[i+NumArtefacts,m]/(uncertainties[i]**2)  for i in range(0,N)])\n",
    "            wp = np.concatenate([np.zeros(NumArtefacts),wp])\n",
    "            \n",
    "            if sum(wp) != 0:\n",
    "                w = np.array([wp/sum(wp)]).T\n",
    "                c = np.mean(np.linalg.multi_dot([X.T,linalg.inv(U),X]))/np.mean(np.dot(w,w.T))\n",
    "                w = np.sqrt(c)*w\n",
    "            else:\n",
    "                NotDone = 1\n",
    "            \n",
    "            # Calculate beta, CovBeta\n",
    "            [gamma, beta, CovBeta] = calculate_beta(y,U,X,w)\n",
    "            beta_ex = beta*np.array([w_s[:,m]]).T\n",
    "\n",
    "            # Calculate chisq\n",
    "            ChiSq_all[m] = model_A(beta_ex, ave_U)\n",
    "            df = NumMeasurements - NumArtefacts\n",
    "        \n",
    "        if min(ChiSq_all)>chi2.isf(0.05,df):\n",
    "            t -= 1\n",
    "        else:\n",
    "            NotDone = 1\n",
    "\n",
    "    set_index = np.argmin(ChiSq_all)\n",
    "    w_set = w_s[:,set_index]\n",
    "    \n",
    "    wf = np.array([w_set[i+NumArtefacts]/(uncertainties[i]**2)  for i in range(0,N)])\n",
    "    wf = np.concatenate([np.zeros(NumArtefacts),wf])\n",
    "    wf = np.array([wf/sum(wf)]).T\n",
    "    \n",
    "    c = np.mean(np.linalg.multi_dot([X.T,linalg.inv(U),X]))/np.mean(np.dot(wf,wf.T))\n",
    "    wf = np.sqrt(c)*wf\n",
    "    \n",
    "    [gamma, beta, CovBeta] = calculate_beta(y,U,X,wf)\n",
    "    \n",
    "    return(w_set,beta,CovBeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BMA(y,U,t):\n",
    "    # Bayesian model averaging\n",
    "    # Returns: estimates of the values for the measurements (mu), and the biases (alpha), and the uncertainties for each\n",
    "    \n",
    "    N = NumParticipants\n",
    "    \n",
    "    # Extract uncertainties from U\n",
    "    uncertainties = ave_U *1.\n",
    "    \n",
    "    S_N = set((i for i in range(0,N)))\n",
    "    \n",
    "    # List of all subsets of size t in set s of size NumParticipants\n",
    "    ss = [set(i) for i in itertools.combinations(S_N, t)]\n",
    "    Q = len(ss)\n",
    "    \n",
    "    # Convert to array of arrays\n",
    "    St =np.array([list(ss[i]) for i in range(0,Q)])\n",
    "    \n",
    "    P_P = np.zeros([Q],dtype = object)\n",
    "    d_all = np.zeros([N,Q])\n",
    "    alpha_all = np.zeros([N,Q],dtype = object)\n",
    "    var_alpha_M = np.zeros([N,Q],dtype = float)\n",
    "    theta_all = np.zeros([Q])\n",
    "    var_mu_M = np.zeros([Q])\n",
    "    \n",
    "    for m in range(0,Q):\n",
    "        # Weighting vector for set of participants with zero bias\n",
    "        w_s = np.zeros([N+NumArtefacts,])\n",
    "        \n",
    "        for i in range(0,t):      \n",
    "            w_s[NumArtefacts+St[m,i]] = 1.        \n",
    "        \n",
    "        # Inverse uncerts for only non-zero participants\n",
    "        wa = (1-w_s[NumArtefacts:])/(uncertainties**2)\n",
    "        \n",
    "        # Inverse uncerts for zero-bias participants\n",
    "        wp = w_s[NumArtefacts:]/(uncertainties**2) \n",
    "        wp = np.concatenate([np.zeros(NumArtefacts),wp])\n",
    "        \n",
    "        # Weights for zero-bias participants\n",
    "        w = np.array([wp/sum(wp)])\n",
    "        c = np.mean(np.linalg.multi_dot([X.T,linalg.inv(U),X]))/np.mean(np.dot(w,w.T))\n",
    "        w = np.sqrt(c)*w\n",
    "        \n",
    "        # Equation 12\n",
    "        var_mu_M[m] = 1 / sum(wp)\n",
    "        \n",
    "        # Fundamental constant model y_i = theta + e_i\n",
    "        [gamma, beta, CovBeta] = calculate_beta(y,U,X,w.T)\n",
    "        ChiSq = model_B(y, U, X, beta)[0,0]\n",
    "        \n",
    "        theta_all[m] = beta[0:NumArtefacts][:,0]\n",
    "        \n",
    "        # d is used to calculate probabilities\n",
    "        d_all[:,m] = beta[NumArtefacts:][:,0]\n",
    "    \n",
    "        # But biases should be zero for the participants with zero bias\n",
    "        # Equation 13\n",
    "        alpha_all[:,m] = np.array([(1.-w_s[NumArtefacts+i])*(d_all[i,m]) for i in range(0,N)])\n",
    "        \n",
    "        # Equation 14\n",
    "        var_alpha_M[:,m] = np.multiply((1.-w_s[NumArtefacts:]),(uncertainties**2+1/sum(wp)))\n",
    "\n",
    "        # Equation 6\n",
    "        P1_e = np.array([np.exp(-d_all[i,m]**2*wp[i+NumArtefacts]/2.) for i in range(0,N)])\n",
    "        P1 = np.array([np.product(P1_e)])\n",
    "        P2 = np.sqrt(1/np.sum(wp))\n",
    "        P3 = np.sqrt(np.product(np.compress(wp.flat!=0, wp.flat)))\n",
    "        \n",
    "        # Proportional probability of each model\n",
    "        P_P[m] = (P1*P2*P3)[0]\n",
    "    \n",
    "    S_P = np.sum(P_P)\n",
    "    P = (P_P/S_P).T\n",
    "    \n",
    "    # Equation 7\n",
    "    mu = np.sum(theta_all*P)\n",
    "    \n",
    "    # Equation 8\n",
    "    alpha = np.array([(np.sum(alpha_all[i,:]*P)) for i in range(0,N)])\n",
    "    \n",
    "    # Equation 9\n",
    "    v_m = np.array([(var_mu_M[m]+theta_all[m]-mu**2)*P[m] for m in range(0,Q)])\n",
    "    \n",
    "    # Equation 10\n",
    "    v_a = np.array([[(var_alpha_M[i,m]+(alpha_all[i,m]-alpha[i,])**2)*P[m] for i in range(0,N)]for m in range(0,Q)])\n",
    "    \n",
    "    unc_mu = np.sqrt(np.sum(v_m))\n",
    "    unc_alpha = np.array([np.sqrt(np.sum(v_a[:,i])) for i in range(0,N)])\n",
    "      \n",
    "    return(alpha, unc_alpha, mu, unc_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BMA(y,U,t):\n",
    "    N = NumParticipants\n",
    "    \n",
    "    # Bayesian model averaging\n",
    "    y = la.uarray([ureal(y[i],np.sqrt(U[i,i])) for i in range(0,N)])\n",
    "    \n",
    "    S_N = set((i for i in range(0,N)))\n",
    "    \n",
    "    # List of all subsets of size t in set s of size N\n",
    "    ss = [set(i) for i in itertools.combinations(S_N, t)]\n",
    "    Q = len(ss)\n",
    "    \n",
    "    # Convert to array of arrays\n",
    "    St =np.array([list(ss[i]) for i in range(0,Q)])\n",
    "    \n",
    "    P_P = np.zeros([Q],dtype = object)\n",
    "    d_all = np.zeros([N,Q],dtype = object)\n",
    "    theta_all = np.zeros([Q],dtype = object)\n",
    "    alpha_all = np.zeros([N,Q],dtype = object)\n",
    "    var_mu_M = np.zeros([Q],dtype = float)\n",
    "    var_alpha_M = np.zeros([N,Q],dtype = float)\n",
    "    \n",
    "    for m in range(0,Q):\n",
    "        # Weighting vector for set of participants with zero bias\n",
    "        w_s = np.zeros([N,])\n",
    "        \n",
    "        for i in range(0,t):      \n",
    "            w_s[St[m,i]] = 1.   \n",
    "        \n",
    "        # Inverse uncerts for only non-zero participants\n",
    "        wa = np.array([(1-w_s[i])/(uncertainty(y[i])**2) for i in range(0,N)])\n",
    "        \n",
    "        # Inverse uncerts for zero-bias participants\n",
    "        wp = w_s/(y.uncertainty(float)**2)         \n",
    "        \n",
    "        # Weights for zero-bias participants\n",
    "        w = wp/sum(wp) \n",
    "        \n",
    "        # Equation 12\n",
    "        var_mu_M[m] = 1./sum(wp)\n",
    "        \n",
    "        # Fundamental constant model y_i = theta + e_i\n",
    "        theta_all[m] = var_mu_M[m]*np.dot(wp,y)\n",
    "        \n",
    "        # d is used to calculate probabilities\n",
    "        d_all[:,m] = y - theta_all[m]\n",
    "    \n",
    "        # But biases should be zero for the participants with zero bias\n",
    "        # Random variable alpha_i, Equation 13\n",
    "        alpha_all[:,m] = np.array([(1.-w_s[i])*(y[i]-theta_all[m]) for i in range(0,N)])\n",
    "        \n",
    "        # Variance of random variable alpha_i under model M_l, Equation 14\n",
    "        var_alpha_M[:,m] = np.array([(1.-w_s[i])*(uncertainty(y[i])**2+var_mu_M[m]) for i in range(0,N)])\n",
    "        \n",
    "        # Equation 6\n",
    "        P1_e = np.array([exp(-d_all[i,m]**2*wp[i]/2.) for i in range(0,N)])\n",
    "        P1 = np.product(P1_e)\n",
    "        P2 = np.sqrt(var_mu_M[m])  \n",
    "        P3 = np.sqrt(np.product(np.compress(wp.flat!=0, wp.flat)))\n",
    "        \n",
    "        # Proportional probability of each model\n",
    "        P_P[m] = P1*P2*P3\n",
    "    \n",
    "    # Probability from Equation 6\n",
    "    S_P = np.sum(P_P)\n",
    "    P = np.array([P_P[m]/S_P for m in range(0,Q)])\n",
    "    \n",
    "    # Equation 8\n",
    "    alpha = np.array([(np.sum(alpha_all[i]*P)) for i in range(0,N)])\n",
    "    \n",
    "    # Equation 7\n",
    "    mu = np.sum(theta_all*P)\n",
    "    \n",
    "    # Equation 10\n",
    "    v_a=np.array([[(var_alpha_M[i,m]+(value(alpha_all[i,m])-value(alpha[i]))**2)*P[m] for i in range(0,N)]for m in range(0,Q)])\n",
    "\n",
    "    # Equation 9\n",
    "    v_m = np.array([(var_mu_M[m]+(value(theta_all[m])-value(mu))**2)*P[m] for m in range(0,Q)])\n",
    "    \n",
    "    unc_alpha = np.array([np.sqrt(np.sum(v_a[:,i])) for i in range(0,N)])\n",
    "    unc_mu = np.sqrt(np.sum(v_m))\n",
    "    \n",
    "    # Put alpha, unc_alpha into an uncertain array\n",
    "    alpha_f = np.array([value(alpha[i]) for i in range(0,N)])\n",
    "    unc_alpha_f = np.array([value(unc_alpha[i]) for i in range(0,N)])\n",
    "    \n",
    "    mu_f = np.array([value(mu)])\n",
    "    unc_mu_f = np.array([value(unc_mu)])\n",
    "    \n",
    "    return(alpha_f, unc_alpha_f, mu_f, unc_mu_f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
