{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates test data and calculates success rates for BMA using 95% credible intervals, with the new numpy GTC\n",
    "# Saves results in this folder.\n",
    "\n",
    "# Can update the participants function to generate success rates for different combinations of biased and unbiased\n",
    "# participants. See txt file in same folder as this to find other versions of the 'participants' function.\n",
    "\n",
    "code_version = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GTC import *\n",
    "import math\n",
    "import xlwt\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import operator as op\n",
    "import itertools\n",
    "import random\n",
    "from scipy.stats import chi2\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LChS(y,N):\n",
    "    # Largest coherent subset\n",
    "    # To store maximum subset size\n",
    "    lchs = [0,0]\n",
    "    \n",
    "    # Calculate the values of alpha_star and z\n",
    "    alpha_star = 2*(0.05/(N*(N-1)))\n",
    "    z = stats.norm.ppf(1-alpha_star/2)\n",
    "    \n",
    "    # Initialise I to store results. I will contain a 1 in the i,j th position if NMIs i,j are not equivalent, and a 0 if\n",
    "    # they are equivalent\n",
    "    I = np.ones(([N,N]))\n",
    "        \n",
    "    # Iterate through the NMIs twice\n",
    "    for i in range(0,N):\n",
    "        for k in range(0,N):  \n",
    "                \n",
    "            if i != k:\n",
    "                # If i,k correspond to different NMIs\n",
    "                    \n",
    "                # Calculate the lower and upper bounds to t\n",
    "                lower = value(y[i]) - value(y[k]) - z*np.sqrt(uncertainty(y[i])**2 + uncertainty(y[k])**2)\n",
    "                upper = value(y[i]) - value(y[k]) + z*np.sqrt(uncertainty(y[i])**2 + uncertainty(y[k])**2)\n",
    "                \n",
    "                # If lower < 0 < upper update I to show i,k are equivalent\n",
    "                if lower < 0 and upper > 0:\n",
    "                    I[i,k] = 0\n",
    "                \n",
    "            else:\n",
    "                # If we are considering the same NMI, then it is equivalent to itself so set I to 0\n",
    "                I[i,k] = 0\n",
    "    \n",
    "    \n",
    "    # Determine the largest coherent subset for each column\n",
    "    # Create dictionaries to store values, equiv for the NMIs which are equivalent pairwise\n",
    "    equiv = {}\n",
    "\n",
    "    for i in range(0,N):\n",
    "        # Iterate through the rows in the matrix\n",
    "        equiv[i] = []\n",
    "\n",
    "        for k in range(0,N):\n",
    "            # Iterate through the columns\n",
    "\n",
    "            if I[i][k]==0:\n",
    "                # If 0, add to a\n",
    "                equiv[i].append(k)\n",
    "        \n",
    "    lchs = 0 # to store maximum subset size\n",
    "        \n",
    "    for i in range(0,N):\n",
    "        # Iterate through the rows of the matrix\n",
    "        s = []\n",
    "        s.append(i)\n",
    "        rows = list(range(0,N))\n",
    "        stop = False\n",
    "    \n",
    "        while stop == False:\n",
    "            # Until we finish going through all rotations of the list\n",
    "                \n",
    "            # Iterate through the rows of the matrix and if the row is equivalent to all elements in s, add row j to s\n",
    "            for k in rows:\n",
    "                test = 0\n",
    "                for v in s:\n",
    "                    if k not in equiv[v]:\n",
    "                        test += 1\n",
    "                if test == 0:\n",
    "                    s.append(k)\n",
    "                \n",
    "            # Get rid of any repeats in s, and if its length is longer than the largest so far, save it\n",
    "            s2 = set(s)\n",
    "            if len(s2) >= lchs:\n",
    "                lchs = len(s2)\n",
    "        \n",
    "            # Move all the elements in the list rows around\n",
    "            rows = rows[1:] + [rows[0]]\n",
    "                \n",
    "            s = []\n",
    "            s.append(i)\n",
    "        \n",
    "            # If we have tried all possible starting elements, get out of loop\n",
    "            if rows[0] == 0:\n",
    "                stop = True  \n",
    "    \n",
    "    return(lchs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BMA(y,N,t):\n",
    "    # Bayesian model averaging for integrating\n",
    "    import operator as op\n",
    "    import itertools\n",
    "    \n",
    "    S_N = set((i for i in range(0,N)))\n",
    "    \n",
    "    # List of all subsets of size t in set s of size N\n",
    "    ss = [set(i) for i in itertools.combinations(S_N, t)]\n",
    "    Q = len(ss)\n",
    "    \n",
    "    # Convert to array of arrays\n",
    "    St =np.array([list(ss[i]) for i in range(0,Q)])\n",
    "    \n",
    "    P_P = np.zeros([Q],dtype = object)\n",
    "    d_all = np.zeros([N,Q],dtype = object)\n",
    "    theta_all = np.zeros([Q],dtype = object)\n",
    "    alpha_all = np.zeros([N,Q],dtype = object)\n",
    "    var_mu_M = np.zeros([Q],dtype = float)\n",
    "    var_alpha_M = np.zeros([N,Q],dtype = float)\n",
    "    pdf = np.zeros([Q], dtype = float)\n",
    "    w_all = np.zeros([Q,N])\n",
    "    \n",
    "    for m in range(0,Q):\n",
    "        # Weighting vector for set of participants with zero bias\n",
    "        w_s = np.zeros([N,])\n",
    "        \n",
    "        for i in range(0,t):      \n",
    "            w_s[St[m,i]] = 1.   \n",
    "        w_all[m,] = w_s\n",
    "        \n",
    "        # Inverse uncerts for only non-zero participants\n",
    "        wa = np.array([(1-w_s[i])/(uncertainty(y[i])**2) for i in range(0,N)])\n",
    "        \n",
    "        # Inverse uncerts for zero-bias participants\n",
    "        wp = w_s/(y.uncertainty()**2)         \n",
    "        \n",
    "        # Weights for zero-bias participants\n",
    "        w = wp/sum(wp) \n",
    "        \n",
    "        # Equation 12\n",
    "        var_mu_M[m] = 1./sum(wp)\n",
    "        \n",
    "        # Equation 11\n",
    "        theta_all[m] = var_mu_M[m]*np.dot(wp,y)\n",
    "        \n",
    "        # d is used to calculate probabilities\n",
    "        d_all[:,m] = y - theta_all[m]\n",
    "    \n",
    "        # But biases should be zero for the participants with zero bias\n",
    "        # Random variable alpha_i, Equation 13\n",
    "        alpha_all[:,m] = np.array([(1.-w_s[i])*(y[i]-theta_all[m]) for i in range(0,N)])\n",
    "        \n",
    "        # Variance of random variable alpha_i under model M_l, Equation 14\n",
    "        var_alpha_M[:,m] = np.array([(1.-w_s[i])*(uncertainty(y[i])**2+var_mu_M[m]) for i in range(0,N)])\n",
    "        \n",
    "        # Equation 6\n",
    "        P1_e = np.array([exp(-d_all[i,m]**2*wp[i]/2.) for i in range(0,N)])\n",
    "        P1 = np.product(P1_e)\n",
    "        P2 = np.sqrt(var_mu_M[m])  \n",
    "        P3 = np.sqrt(np.product(np.compress(wp.flat!=0, wp.flat)))\n",
    "        \n",
    "        # Proportional probability of each model\n",
    "        P_P[m] = P1*P2*P3\n",
    "    \n",
    "    \n",
    "    # Probability from Equation 6\n",
    "    S_P = np.sum(P_P)\n",
    "    P = np.array([P_P[m]/S_P for m in range(0,Q)])\n",
    "    \n",
    "    # Equation 7\n",
    "    mu = np.sum(theta_all*P)\n",
    "    \n",
    "    # Equation 8\n",
    "    alpha = np.array([(np.sum(alpha_all[i]*P)) for i in range(0,N)])\n",
    "    \n",
    "    # Equation 9\n",
    "    v_m = np.array([(var_mu_M[m]+(value(theta_all[m])-value(mu))**2)*P[m] for m in range(0,Q)])\n",
    "    \n",
    "    # Equation 10\n",
    "    v_a=np.array([[(var_alpha_M[i,m]+(value(alpha_all[i,m])-value(alpha[i]))**2)*P[m] for i in range(0,N)]for m in range(0,Q)])\n",
    "    \n",
    "    unc_alpha = np.array([np.sqrt(value(np.sum(v_a[:,i]))) for i in range(0,N)])\n",
    "    \n",
    "    alpha = la.uarray([ureal(value(alpha[i]),value(unc_alpha[i])) for i in range(0,len(alpha))])\n",
    "    \n",
    "    return(theta_all, var_mu_M, P, Q, w_all, alpha, unc_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf(a, theta_all, var_mu_M, P, Q, y, w_all, i):\n",
    "    # Sum up gaussian pdfs for alpha\n",
    "    pdf_sum = 0\n",
    "    for l in range(0,Q):\n",
    "        w_l = w_all[l,]\n",
    "        if w_l[i] != 1:\n",
    "            pdf_sum += value(P[l]) * np.exp(-(a-(value(y[i])-value(theta_all[l])))**2/(2*(uncertainty(y[i])**2+var_mu_M[l]))) / np.sqrt(2*np.pi*(uncertainty(y[i])**2+var_mu_M[l]))\n",
    "    return pdf_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_function(Q,w_all,i,P):\n",
    "    # Sum up the delta functions for alpha\n",
    "    delta_sum = 0\n",
    "    for l in range(0,Q):\n",
    "        w_l = w_all[l,]\n",
    "        if w_l[i] == 1:\n",
    "            delta_sum += value(P[l])\n",
    "    return delta_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_function(a,mu,sigma,theta_all,var_mu_M,P,Q,y,w_all,i):\n",
    "    # Calculate the area under the probability density function between mu-a and mu+a\n",
    "    if mu-a > 0:\n",
    "        area = scipy.integrate.quad(pdf,mu-a,mu+a,args=(theta_all,var_mu_M,P,Q,y,w_all,i))[0]\n",
    "    elif mu+a < 0:\n",
    "        area = scipy.integrate.quad(pdf,mu-a,mu+a,args=(theta_all,var_mu_M,P,Q,y,w_all,i))[0]\n",
    "    else:\n",
    "        # If it crosses zero, add delta function\n",
    "        delta_sum = delta_function(Q,w_all,i,P)\n",
    "        area = delta_sum + scipy.integrate.quad(pdf,mu-a,mu+a,args=(theta_all,var_mu_M,P,Q,y,w_all,i))[0]\n",
    "    return area\n",
    "\n",
    "def bisection(f,N,tol,theta_all,var_mu_M,P,Q,y,w_all,i, alpha_mu, alpha_var):\n",
    "    # Use above function to determine the value of a which gives a 95% confidence interval\n",
    "    mu = value(alpha_mu[i])\n",
    "    sigma = value(alpha_var[i])\n",
    "\n",
    "    a = 0\n",
    "    b = 20\n",
    "    c = (b-a)/2\n",
    "\n",
    "    for n in range(1,N+1):\n",
    "        if 0.95 - f(c,mu,sigma,theta_all,var_mu_M,P,Q,y,w_all,i) < -tol:\n",
    "            if (b-a)/2 < tol:\n",
    "                c = c+0.1\n",
    "                return c, f(c,mu,sigma,theta_all,var_mu_M,P,Q,y,w_all,i)\n",
    "            \n",
    "            b = c*1.\n",
    "            c = a + (b-a)/2\n",
    "            \n",
    "        elif 0.95 - f(c,mu,sigma,theta_all,var_mu_M,P,Q,y,w_all,i) > tol:\n",
    "            if (b-a)/2 < tol:\n",
    "                c = c+0.1\n",
    "                return c, f(c,mu,sigma,theta_all,var_mu_M,P,Q,y,w_all,i)\n",
    "            a = c*1.\n",
    "            c = a + (b-a)/2\n",
    "        \n",
    "        elif b-a < tol:\n",
    "            c = c+0.1\n",
    "            return c, f(c,mu,sigma,theta_all,var_mu_M,P,Q,y,w_all,i)\n",
    "        \n",
    "        else:\n",
    "            return c, f(c,mu,sigma,theta_all,var_mu_M,P,Q,y,w_all,i)\n",
    "    \n",
    "    c = c+0.1\n",
    "    return c, f(c,mu,sigma,theta_all,var_mu_M,P,Q,y,w_all,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One biased participant\n",
    "\n",
    "def participants(N,f,g,d):\n",
    "    # Generate a participant set, run the methods and return the required variables to determine success\n",
    "    # Takes in: N = number of participants\n",
    "    #           f = uncertainty of the biased lab\n",
    "    #           g = size of bias\n",
    "    #           d = degrees of freedom of Chi Square distribution used to generated uncertainties\n",
    "    # Returns:  y = uncertain array containing N participants, the first of which has a bias defined by f,g\n",
    "    \n",
    "    unc = np.array([np.random.chisquare(d)/d for i in range(0,N-1)])\n",
    "    unc = np.insert(unc, 0, f) # Set the first lab to have f=1\n",
    "\n",
    "    # Generate errors from a Normal (0,unc) distribution\n",
    "    err = np.array([random.normalvariate(0,unc[i]) for i in range(1,N)])\n",
    "    err = np.insert(err, 0, [random.normalvariate(0,unc[0])+g]) # Set the first lab to have a bias of 6*unc\n",
    "\n",
    "    # Put 12 participants into an uncertain number array\n",
    "    y = la.uarray([ureal(err[i],unc[i]) for i in range(0,N)])\n",
    "    \n",
    "    n_biased = 1\n",
    "    \n",
    "    return(y, n_biased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def simulate_data(T,N,f,g,df):\n",
    "    # Generate datasets, and tests them to see how many labs fail using each method\n",
    "    \n",
    "    # Create array in which to store the failure rates, and m\n",
    "    fail = np.zeros((N))\n",
    "    m_sum = 0\n",
    "    \n",
    "    # Initialise stop (to break out of while loop), t_ave\n",
    "    stop = T * 1.\n",
    "    t_ave = 0\n",
    "\n",
    "    while stop > 0:\n",
    "        print(stop)\n",
    "        # Create a set of N participants\n",
    "        [y, n_bias] = participants(N,f,g,df)\n",
    "        \n",
    "        # Find size of largest coherent subset\n",
    "        lchs = LChS(y,N)\n",
    "        \n",
    "        # Set m to be lchs - 1\n",
    "        m = lchs - 1\n",
    "        m_sum += m\n",
    "        \n",
    "        # Run participant set through BMA\n",
    "        [theta_all, var_mu_M, P, Q, w_all, alpha, unc_alpha] = BMA(y,N,m)\n",
    "        \n",
    "        # Create temporary arrays to store results\n",
    "        k_temp = np.zeros(N)\n",
    "        coverage_temp = np.zeros(N)\n",
    "        \n",
    "         # Calculate coverage factor for each participant\n",
    "        for j in range(0,N):           \n",
    "            # Calculate delta function value\n",
    "            delta = delta_function(Q,w_all,j,P)\n",
    "\n",
    "            # Calculate k and coverage value\n",
    "            [k_temp[j], coverage_temp[j]] = bisection(search_function,100,0.001,theta_all,var_mu_M,P,Q,y,w_all,j,alpha,unc_alpha)\n",
    "\n",
    "            # Determine failure using k\n",
    "            if abs(alpha[j]) > k_temp[j]:\n",
    "                fail[j] += 1\n",
    "        \n",
    "        # Update stop\n",
    "        stop -= 1\n",
    "    \n",
    "    # Calculate fail rates by dividing the number of labs which failed by the number of trials\n",
    "    fail_rate = fail/T\n",
    "    m_ave = m_sum / T\n",
    "    \n",
    "    return(fail_rate, m_ave, n_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "N = 12\n",
    "T = 1\n",
    "df = 4\n",
    "\n",
    "g_all = [2.5]\n",
    "f_all = [1]\n",
    "\n",
    "for f in f_all:\n",
    "    for g in g_all:\n",
    "\n",
    "        # Initliase spreadsheet to save results\n",
    "        wb = xlwt.Workbook()\n",
    "        ws = wb.add_sheet('Fail rate')\n",
    "\n",
    "        # Generate failure rates\n",
    "        [fail_rates, m_ave, n_biased] = simulate_data(T,N,f,g,df)\n",
    "        \n",
    "        if n_biased == 1:\n",
    "            ws.write(0,0,'Biased')\n",
    "        elif n_biased == 2:\n",
    "            ws.write(0,0,'Biased')\n",
    "            ws.write(0,1,'Biased')\n",
    "\n",
    "        # Write results to spreadsheet\n",
    "        for i in range(0,N):\n",
    "            ws.write(1,i,fail_rates[i])\n",
    "\n",
    "        ws.write(0, 13, 'f')\n",
    "        ws.write(0, 14, f)\n",
    "        ws.write(1, 13, 'g')\n",
    "        ws.write(1, 14, g)\n",
    "        ws.write(2, 13, 'Average m')\n",
    "        ws.write(2, 14, m_ave)\n",
    "        ws.write(3, 13, 'Code version')\n",
    "        ws.write(3, 14, code_version)\n",
    "\n",
    "        wb.save('2bias,+-,BMA-lchs-1,f='+str(f)+',g='+str(g)+'.xls')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
